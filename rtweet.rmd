---
title: "rtweet"
subtitle: "Scraping Tweets Using Twitters Public API"
author: "Tillman Degens"

date: 
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: "metropolis"
    font-size: 40px
  
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---    

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load(tidyverse, dplyr, rtweet, httpuv, xaringan, quantmod, tidyquant, ggplot2, lubridate,zoo, stringr, tidytext, jsonlite, httr, listviewer, usethis, fredr)
```

# rtweet

## What it is
- Collect and organize Twitter data using Twitter's Public API

- Analyze Tweet contents, trends, interactions, and locations

## Getting set up

- First sign up for a twitter developer account and create a project

- Next load your API token

- Start pulling in Tweets

---

# Set up the API key

## What you do

```{r,eval=FALSE}
api_key = "Your Key"
api_secret = "Your Secret Key"
acess_token = "Your Token"
token_secret = "Your Secret Token"

token = create_token(
  app = "rtweet510",
  consumer_key = api_key,
  consumer_secret = api_secret,
  access_token = acess_token,
  access_secret = token_secret)

```

```{r,include=FALSE}
api_key = "P4O8ScNCkqyYh9b20Tpj6egrD"
api_secret = "EOqAKdWQNz0xTm9YRIl97WdsYtDovwQZbhccmGoj4wi8I5nf9G"
acess_token = "1035359420526383105-RXUIArIibIpUR9xgBi6ZeHMwVqtr3d"
token_secret = "gmK0Ccl7Kk5jGwo1fpC0SIBtI7VWWU5fjhxTYtAkW36sH"

token = create_token(
  app = "rtweet510",
  consumer_key = api_key,
  consumer_secret = api_secret,
  access_token = acess_token,
  access_secret = token_secret
)

```
## What goes on Github

``` {r echo = T, results = 'hide'}
get_token()
```
---

# On the Most Basic Level: search_tweets

``` {r}
Bioline_df = search_tweets("#BLRX", n = 5000, include_rts = FALSE)
```

- Specify that we are searching for Tweets containing #BLRX

- This gives us a data frame with 90 rows, only some of them are useful!

- Only returns Tweets from past 6-9 days (hence Cron Jobs)

- "Limit" of 18,000 tweets

---

# Let's Select some Variables

``` {r}
Bioline_df = Bioline_df %>%
  select(screen_name, created_at, favorite_count, followers_count, location, status_id, user_id)
head(Bioline_df)
```

- Think of the GME short squeeze!

- Large financial firms are looking for people who can spot trends among retail investors

---
# Bioline Interactions
``` {r, echo = FALSE}
ts_plot(Bioline_df, "4 hours") +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets with a #BLRX",
       subtitle = paste0(format(min(Bioline_df$created_at), "%d %B %Y"), " to ", format(max(Bioline_df$created_at),"%d %B %Y")),
       caption = "Data collected from Twitter's REST API via rtweet") +
  theme_minimal()
```
---
# Some Important Things to Consider

- Tweets are rapidly changing, you will probably want to save to .csv

``` {r, eval = FALSE}
write_as_csv(Bioline_df, "Bioline.csv")
```

- People are not expecting us to use their tweets

- Twitter is not the real world. Be careful inferring relationships based on Tweets

- Twitter users skew towards college educated male democrats

- 80 percent of Tweets are generated by 10% of users

---
# User Data

- Taking a look at who's following me
``` {r}
Tillman_followers = get_followers("DegensTillman")
head(Tillman_followers)
```
---
# Great for looking at your own Twitter data
``` {r}
Tillman = get_mentions()
Tillman = Tillman %>% select(created_at, text, favorite_count)
Tillman
```
---

# Stream Tweets

- Stream a random sample of all live tweets: 

```{r}
trending = stream_tweets("")
```
---
# Most Used Hashtags
```{r}
trending %>% 
  unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(hashtag, "^#"))%>%
  count(hashtag, sort = TRUE) %>%
  top_n(10)
```

---

#Use cases
- Politics

- Finance

- Social media/machine learning

- Consumer analytics

- And so many more!!